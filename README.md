# ğŸš€ SpaceX Data Pipeline

This repository contains an **end-to-end data pipeline** built to extract, transform, and load (ETL) SpaceX launch data using modern data engineering tools and best practices. The pipeline is designed to be **production-ready**, **containerized**, and **cloud-deployable** using Google Cloud Platform (GCP).

---

## ğŸ“Œ Project Overview

This project automates the following ETL workflow:

1. **ğŸ”„ Data Ingestion**: Fetches raw SpaceX launch data from the [SpaceX public API](https://api.spacexdata.com/v4/launches).
2. **ğŸ§¹ Data Cleansing & Validation**: Uses Python to process and validate the raw data.
3. **ğŸ“¦ Data Loading**: Uploads the clean data to **Google Cloud Storage (GCS)** and loads it into **BigQuery**.
4. **ğŸ” Data Transformation**: Applies further modeling and transformations with **dbt** (Data Build Tool).
5. **ğŸ•¹ï¸ Orchestration**: Uses **Apache Airflow** to schedule and manage the entire pipeline through DAGs.
6. **â˜ï¸ Cloud Deployment**: Entire pipeline is **containerized with Docker**, deployed on **Cloud Run**, and triggered daily via **Cloud Scheduler**.


## ğŸ› ï¸ Tech Stack

| Layer                | Tools/Tech                            |
|---------------------|----------------------------------------|
| Orchestration       | Apache Airflow                         |
| Data Processing     | Python (Pandas, Requests, etc.)        |
| Data Warehouse      | Google BigQuery                        |
| Data Storage        | Google Cloud Storage                   |
| Transformation      | dbt (BigQuery adapter)                 |
| Containerization    | Docker                                 |
| Deployment          | Cloud Run + Cloud Scheduler            |
| Infrastructure      | Google Cloud Platform (GCP)            |

---

## ğŸ“ Project Structure

SpaceX-Data-Pipeline/
â”‚
â”œâ”€â”€ dags/ # Airflow DAGs
â”‚ â””â”€â”€ spacex_etl_dag.py
â”‚
â”œâ”€â”€ scripts/ # Python scripts for ingestion & processing
â”‚ â”œâ”€â”€ fetch_spacex_data.py
â”‚ â”œâ”€â”€ clean_spacex_data.py
â”‚ â””â”€â”€ upload_to_gcs.py
â”‚
â”œâ”€â”€ dbt/ # dbt project folder
â”‚ â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ dbt_project.yml
â”‚ â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ Dockerfile # Container for the entire ETL pipeline
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ ...
